{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3337128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1, opens a data file in csv, and transform it into a usable format \n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    data_Set = pd.read_csv(open(\"data/student.csv\"))         # data transformation to usable format using pandas\n",
    "    \n",
    "    return data_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b92dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, splits a data set into a training set and hold-out test set\n",
    "def split_data():\n",
    "    data_Set = pd.read_csv(open(\"data/student.csv\"))\n",
    "    \n",
    "    train_Size = 300\n",
    "    test_Size  = 649 - train_Size                           # define split size\n",
    "    \n",
    "    train_Data = data_Set.iloc[0:train_Size]                # define training data set\n",
    "    test_Data  = data_Set.iloc[train_Size:]                 # define testing  data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e49d6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3, builds a supervised NB model from training data\n",
    "def train():\n",
    "    data_Set = pd.read_csv(open(\"data/student.csv\"))\n",
    "    \n",
    "    train_Size = 300\n",
    "    test_Size  = 649 - train_Size\n",
    "    \n",
    "    train_Data = data_Set.iloc[0:train_Size]                # train data set, 0~299 instances\n",
    "    test_Data  = data_Set.iloc[train_Size:]                 # test data set, 300~649 instances\n",
    "    \n",
    "    dic_Att = {}                                            # dictionary for attributes \n",
    "    attributes = train_Data.columns                         # attribute names\n",
    "    \n",
    "    for x in attributes:\n",
    "        attributes_Count = train_Data.groupby('Grade')[x].value_counts()  # the number of each grade\n",
    "        dic_Att[x] = train_Data.groupby('Grade')[x].value_counts()        # the number of each value\n",
    "    \n",
    "    return dic_Att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb73caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4, predicts the class for an instance or a set of instances, based on a trained model \n",
    "def predict():\n",
    "    data_Set = pd.read_csv(open(\"data/student.csv\"))\n",
    "    data = open(\"data/student.csv\").read()\n",
    "    datalines = data.split('\\n')\n",
    "    datafields = []\n",
    "    \n",
    "    for line in datalines:\n",
    "        datafields.append(line.split(\",\"))\n",
    "    \n",
    "    train_Size = 300\n",
    "    test_Size  = 649 - train_Size\n",
    "        \n",
    "    train_Data = data_Set.iloc[0:train_Size]\n",
    "    test_Data  = data_Set.iloc[train_Size:]\n",
    "    \n",
    "    data_Grade = data_Set[\"Grade\"]                          # value of grade attribute\n",
    "    grade_Count = data_Grade.value_counts()                 # count each grades A+ ~ F\n",
    "    \n",
    "    attributes = train_Data.columns[0:29]                   # each attribute names\n",
    "    attributes_1 = train_Data.columns[0:22]                 # divide the attribute names because some values are integer\n",
    "    attributes_2 = train_Data.columns[22:28]\n",
    "    attributes_3 = train_Data.columns[28:29]\n",
    "    dic_Att = {}\n",
    "    pred_List = []\n",
    "    init = 0\n",
    "    \n",
    "    for x in attributes:                                                    # training the model\n",
    "        attributes_Count = data_Set.groupby('Grade')[x].value_counts()      # the number of each grade\n",
    "        dic_Att[x] = data_Set.groupby('Grade')[x].value_counts()            # the number of each value\n",
    "\n",
    "    dic_Grade_Count = {'A': 0 , 'A+': 0, 'B': 0, 'C': 0, 'D': 0, 'F': 0}\n",
    "    grade = ['A', 'A+', 'B', 'C', 'D', 'F']\n",
    "    for a in grade:                                                         # count the number of each grade\n",
    "        dic_Grade_Count[a] = grade_Count[a]                                 # it will be the denominator of calculation.\n",
    "        \n",
    "    pred_Dic = {'A': 0 , 'A+': 0, 'B': 0, 'C': 0, 'D': 0, 'F': 0}\n",
    "    grade = ['A', 'A+', 'B', 'C', 'D', 'F']\n",
    "    pred_Arr = []\n",
    "    \n",
    "    for w in range(1, 650):                                            # predict the test data set\n",
    "        for y in grade:                                                # for all grades\n",
    "            init = 0\n",
    "            sum = 1\n",
    "            for x in attributes_1:                                     # multiply each probability of value\n",
    "                if datafields[w][init] not in dic_Att[x][y].keys():\n",
    "                    sum = sum * 0.0001                                 # epsilon is 0.0001\n",
    "                else:\n",
    "                    sum = sum * dic_Att[x][y][datafields[w][init]] / dic_Grade_Count[y]\n",
    "                init = init + 1\n",
    "            for x in attributes_2:                                     # change format from str to int to calculate\n",
    "                if int(datafields[w][init]) not in dic_Att[x][y].keys():\n",
    "                    sum = sum * 0.0001\n",
    "                else:\n",
    "                    sum = sum * dic_Att[x][y][int(datafields[w][init])] / dic_Grade_Count[y]\n",
    "                init = init + 1\n",
    "            for x in attributes_3:\n",
    "                if datafields[w][init] not in dic_Att[x][y].keys():\n",
    "                    sum = sum * 0.0001\n",
    "                else:\n",
    "                    sum = sum * dic_Att[x][y][datafields[w][init]] / dic_Grade_Count[y]\n",
    "                init = init + 1\n",
    "            pred_Dic[y] = sum * dic_Grade_Count[y] / len(data_Set)       # calculate probability of each grades \n",
    "            Max = 'A'\n",
    "        for i in grade:                                                  # take the maximum among A+ ~ F\n",
    "            if pred_Dic[i] > pred_Dic['A']:\n",
    "                Max = i\n",
    "        pred_Arr.append(Max)                                             # return the best probability of grade\n",
    "    return pred_Arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "499a70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5, evaluates a set of predictions in terms of accuracy\n",
    "def evaluate():\n",
    "    data_Set = pd.read_csv(open(\"data/student.csv\"))\n",
    "    \n",
    "    train_Size = 300\n",
    "    test_Size  = 649 - train_Size\n",
    "    pred_Arr = predict()\n",
    "    \n",
    "    correct = 0\n",
    "    wrong   = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for i in range(train_Size, 649):\n",
    "        if pred_Arr[i] == data_Set['Grade'][i]:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "    \n",
    "    accuracy = correct / (correct + wrong)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "731f3878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.332378223495702\n"
     ]
    }
   ],
   "source": [
    "# Step 6, Accuracy\n",
    "print(\"Accuraccy:\", evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
